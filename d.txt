Assignment 7:



from pyspark import SparkContext

sc = SparkContext("local", "WordCount")

# Load text file into an RDD
text_rdd = sc.textFile("input.txt")

# Map Phase: Split lines into words and assign count 1 to each word
words_rdd = text_rdd.flatMap(lambda line: line.split(" ")) \
                     .map(lambda word: (word, 1))

# Reduce Phase: Sum counts of the same words
word_count_rdd = words_rdd.reduceByKey(lambda x, y: x + y)

# Collect and print results
print(word_count_rdd.collect())

sc.stop()




Explanation:


This code performs a **word count** using **PySpark's low-level RDD API**. It's one of the most basic and classic examples in distributed computing.

Here‚Äôs a **step-by-step explanation** of what it does:

---

### üî∂ **1. Import and Start Spark Context**
```python
from pyspark import SparkContext

sc = SparkContext("local", "WordCount")
```
- `SparkContext` is the entry point for Spark functionality when working with **RDDs**.
- `"local"` means it's running on your local machine (not a cluster).
- `"WordCount"` is the application name.

---

### üî∂ **2. Load Text File**
```python
text_rdd = sc.textFile("input.txt")
```
- Loads `input.txt` into an RDD (Resilient Distributed Dataset).
- Each line in the file becomes one element in `text_rdd`.

---

### üî∂ **3. Map Phase ‚Äì Tokenize and Assign Initial Count**
```python
words_rdd = text_rdd.flatMap(lambda line: line.split(" ")) \
                     .map(lambda word: (word, 1))
```
- `flatMap(...)`: Splits each line into words. Flattens the result into a single list of words.
- `map(...)`: Converts each word into a tuple like `("word", 1)`.

üß† This prepares the data for **counting**, assigning `1` to each occurrence.

---

### üî∂ **4. Reduce Phase ‚Äì Count Word Frequencies**
```python
word_count_rdd = words_rdd.reduceByKey(lambda x, y: x + y)
```
- Groups all tuples by the **word** (the key).
- For each group, adds the counts together (i.e., `1 + 1 + ...`).
- Result is a new RDD like: `[("word1", count1), ("word2", count2), ...]`

---

### üî∂ **5. Collect and Display Results**
```python
print(word_count_rdd.collect())
```
- `collect()` brings all results from the RDD back to the **driver** as a list.
- `print(...)` displays the word counts.

---

### üî∂ **6. Stop Spark Context**
```python
sc.stop()
```
- Gracefully shuts down the Spark context and releases resources.

---

### ‚úÖ **Example Output**
Suppose `input.txt` contains:
```
hello world
hello spark
```
Then the output will be something like:
```python
[('hello', 2), ('world', 1), ('spark', 1)]
```

---

Let me know if you want the same logic done using **PySpark DataFrame API** instead of RDDs!












Assignment 8:



from pyspark.sql import SparkSession

# Start Spark
spark = SparkSession.builder.appName("MatrixMul").getOrCreate()
sc = spark.sparkContext

# Input matrices (A: 2x3, B: 3x2)
A = [(0, 0, 4), (0, 1, 6), (0, 2, 8),
     (1, 0, 5), (1, 1, 5), (1, 2, 4)]

B = [(0, 0, 7), (0, 1, 8),
     (1, 0, 9), (1, 1, 10),
     (2, 0, 11), (2, 1, 12)]

# Parallelize
rddA = sc.parallelize(A)
rddB = sc.parallelize(B)

# Map and join
a_mapped = rddA.map(lambda x: (x[1], (x[0], x[2])))
b_mapped = rddB.map(lambda x: (x[0], (x[1], x[2])))
joined = a_mapped.join(b_mapped)

# Multiply and reduce
products = joined.map(lambda x: ((x[1][0][0], x[1][1][0]), x[1][0][1] * x[1][1][1]))
result = products.reduceByKey(lambda x, y: x + y)

# Show result
for ((i, j), val) in sorted(result.collect()):
    print(f"({i}, {j}) -> {val}")

spark.stop()






Explanation:


This PySpark code performs **matrix multiplication** using **RDDs**. Let‚Äôs walk through it step-by-step with detailed explanation and visuals.

---

### üî∑ **Overview**

- Matrix A is **2√ó3**
- Matrix B is **3√ó2**
- The result of `A √ó B` is a **2√ó2** matrix

---

### üî∂ **1. Start Spark Session**
```python
from pyspark.sql import SparkSession
spark = SparkSession.builder.appName("MatrixMul").getOrCreate()
sc = spark.sparkContext
```
- Starts a Spark session named **"MatrixMul"**
- Gets the Spark **context** (for working with RDDs)

---

### üî∂ **2. Input Matrices**
```python
A = [(0, 0, 4), (0, 1, 6), (0, 2, 8),
     (1, 0, 5), (1, 1, 5), (1, 2, 4)]

B = [(0, 0, 7), (0, 1, 8),
     (1, 0, 9), (1, 1, 10),
     (2, 0, 11), (2, 1, 12)]
```

Each tuple is in the form: `(row_index, col_index, value)`

So:
```
Matrix A =
[[4, 6, 8],
 [5, 5, 4]]

Matrix B =
[[ 7,  8],
 [ 9, 10],
 [11, 12]]
```

---

### üî∂ **3. Parallelize to RDDs**
```python
rddA = sc.parallelize(A)
rddB = sc.parallelize(B)
```
- Converts Python lists to **distributed RDDs**

---

### üî∂ **4. Map to Prepare for Join**
```python
a_mapped = rddA.map(lambda x: (x[1], (x[0], x[2])))
b_mapped = rddB.map(lambda x: (x[0], (x[1], x[2])))
```
We‚Äôre **mapping by the common dimension** (`k`) for matrix multiplication:

- `a_mapped`: keyed by **A‚Äôs column** ‚Üí `(k, (i, A[i][k]))`
- `b_mapped`: keyed by **B‚Äôs row** ‚Üí `(k, (j, B[k][j]))`

Why? Because we multiply `A[i][k] * B[k][j]`, and `k` is common in both.

---

### üî∂ **5. Join on the Shared Index `k`**
```python
joined = a_mapped.join(b_mapped)
```
- Joins A and B entries with the **same `k`**
- Output is: `(k, ((i, A_val), (j, B_val)))`

---

### üî∂ **6. Multiply Corresponding Values**
```python
products = joined.map(lambda x: ((x[1][0][0], x[1][1][0]), x[1][0][1] * x[1][1][1]))
```
- Computes: `A[i][k] * B[k][j]`
- Key becomes: `(i, j)` for result matrix `C[i][j]`
- Value: product of the two numbers

---

### üî∂ **7. Reduce to Sum Partial Products**
```python
result = products.reduceByKey(lambda x, y: x + y)
```
- Adds all the partial products for each `(i, j)`
- Final value is `C[i][j]`

---

### üî∂ **8. Display Results**
```python
for ((i, j), val) in sorted(result.collect()):
    print(f"({i}, {j}) -> {val}")
```
- Collects results to the driver and prints them in order.

---

### ‚úÖ **Expected Output**
For the input matrices A (2√ó3) and B (3√ó2), result C (2√ó2):

```
C[0][0] = 4√ó7 + 6√ó9 + 8√ó11 = 28 + 54 + 88 = 170
C[0][1] = 4√ó8 + 6√ó10 + 8√ó12 = 32 + 60 + 96 = 188
C[1][0] = 5√ó7 + 5√ó9 + 4√ó11 = 35 + 45 + 44 = 124
C[1][1] = 5√ó8 + 5√ó10 + 4√ó12 = 40 + 50 + 48 = 138
```

So output:
```
(0, 0) -> 170
(0, 1) -> 188
(1, 0) -> 124
(1, 1) -> 138
```

---







Assignment 9:



!pip install -q pandas requests pyspark


# ----------------- STEP 0: INSTALL DEPENDENCIES -----------------


import os
import requests
import pandas as pd
from pyspark.sql import SparkSession, functions as F
from pyspark.sql.functions import col
from pyspark.ml.regression import LinearRegression
from pyspark.ml.feature import VectorAssembler

# ----------------- 1. DOWNLOAD WEATHER DATA -----------------
base_url = "https://www.ncei.noaa.gov/data/global-summary-of-the-day/access/{}/{}.csv"
years = range(2021, 2024)
stations = ["99495199999", "72429793812"]  # Florida & Cincinnati

input_dir = "/content/weather_data"
output_dir = "/content/cleaned_weather_data"

for year in years:
    os.makedirs(f"{input_dir}/{year}", exist_ok=True)
    for station in stations:
        url = base_url.format(year, station)
        response = requests.get(url)
        if response.status_code == 200:
            with open(f"{input_dir}/{year}/{station}.csv", "wb") as file:
                file.write(response.content)

# ----------------- 2. CLEAN DATA -----------------
invalid_values = {"MAX": 9999.9, "MXSPD": 999.9}

for year in years:
    os.makedirs(f"{output_dir}/{year}", exist_ok=True)
    for station in stations:
        file_path = f"{input_dir}/{year}/{station}.csv"
        df = pd.read_csv(file_path)
        for column, invalid in invalid_values.items():
            if column in df.columns:
                df = df[df[column] != invalid]
        df.to_csv(f"{output_dir}/{year}/{station}.csv", index=False)

# ----------------- 3. SPARK SETUP -----------------
spark = SparkSession.builder.appName("Weather Analysis").getOrCreate()

# ----------------- 4. HOTTEST DAY -----------------
hottest_days = {}
for year in years:
    for station in stations:
        path = f"{output_dir}/{year}/{station}.csv"
        df = spark.read.csv(path, header=True, inferSchema=True)
        if "MAX" in df.columns:
            max_day = df.orderBy(F.desc("MAX")).first()
            if max_day:
                hottest_days[year] = (max_day.STATION, max_day.DATE, max_day.MAX)

hottest_days_df = spark.createDataFrame([(year, *data) for year, data in hottest_days.items()],
                                        ["YEAR", "STATION", "DATE", "MAX"])
print("üî• Hottest Days")
hottest_days_df.show()

# ----------------- 5. COLDEST DAY IN MARCH -----------------
march_data = []
for year in years:
    for station in stations:
        path = f"{output_dir}/{year}/{station}.csv"
        df = spark.read.csv(path, header=True, inferSchema=True)
        if "MIN" in df.columns and "DATE" in df.columns:
            march_df = df.filter(df.DATE.contains('-03-'))
            coldest_day = march_df.orderBy(F.asc("MIN")).first()
            if coldest_day:
                march_data.append((coldest_day.STATION, coldest_day.DATE, coldest_day.MIN))

coldest_day_df = spark.createDataFrame(march_data, ["STATION", "DATE", "MIN"])
print("‚ùÑÔ∏è Coldest Days in March")
coldest_day_df.show()

# ----------------- 6. AVERAGE PRECIPITATION -----------------
annual_precipitation = []
for year in years:
    for station in stations:
        path = f"{output_dir}/{year}/{station}.csv"
        df = spark.read.csv(path, header=True, inferSchema=True)
        if "PRCP" in df.columns:
            mean_prcp = df.agg(F.mean("PRCP").alias("Mean_PRCP")).first().Mean_PRCP
            annual_precipitation.append((station, year, mean_prcp))

annual_precipitation_df = spark.createDataFrame(annual_precipitation, ["STATION", "YEAR", "Mean_PRCP"])
print("üåßÔ∏è Average Precipitation")
annual_precipitation_df.show()

# ----------------- 7. WIND CHILL ANALYSIS -----------------
path = f"{output_dir}/2022/72429793812.csv"
df = spark.read.csv(path, header=True, inferSchema=True)
if "TEMP" in df.columns and "WDSP" in df.columns:
    df_filtered = df.filter((col("TEMP") < 50) & (col("WDSP") > 3))
    df_wind_chill = df_filtered.withColumn("Wind Chill",
        35.74 + (0.6215 * col("TEMP")) - (35.75 * (col("WDSP") ** 0.16)) +
        (0.4275 * col("TEMP") * (col("WDSP") ** 0.16))
    )
    print("üí® Wind Chill")
    df_wind_chill.select("DATE", "Wind Chill").orderBy("Wind Chill").show()

# ----------------- 8. TEMPERATURE PREDICTION MODEL -----------------
df = spark.read.csv(path, header=True, inferSchema=True)
if "MAX" in df.columns and "DATE" in df.columns:
    df = df.withColumn("DAY_OF_YEAR", F.dayofyear("DATE"))
    assembler = VectorAssembler(inputCols=["DAY_OF_YEAR"], outputCol="features")
    train_data = assembler.transform(df).select("features", col("MAX").alias("label"))

    lr = LinearRegression()
    lr_model = lr.fit(train_data)

    predictions_df = spark.createDataFrame([(day,) for day in range(305, 366)], ["DAY_OF_YEAR"])
    predictions = assembler.transform(predictions_df)
    predicted_temps = lr_model.transform(predictions)

    print("üìà Predicted Temperatures (Late Year)")
    predicted_temps.select("DAY_OF_YEAR", "prediction").show()

# ----------------- 9. EXTREME WEATHER DAYS -----------------
path = f"{output_dir}/2023/99495199999.csv"
df = spark.read.csv(path, header=True, inferSchema=True)
if "FRSHTT" in df.columns:
    extreme_weather_count = df.filter(col("FRSHTT") != 0).count()
    print(f"üå™Ô∏è Number of extreme weather days in 2023: {extreme_weather_count}")







Explanation:


Here‚Äôs a breakdown and explanation of the provided PySpark + Pandas code step by step:

---

### **STEP 0: INSTALL DEPENDENCIES**
These are installed earlier in Google Colab using:
```bash
pip install pandas requests pyspark
```
Then the necessary Python libraries are imported:
- `os`: For directory creation and file management
- `requests`: For downloading data from a URL
- `pandas`: For initial data cleaning
- `pyspark`: For distributed processing and ML modeling

---

### **1. DOWNLOAD WEATHER DATA**
```python
base_url = "https://www.ncei.noaa.gov/data/global-summary-of-the-day/access/{}/{}.csv"
years = range(2021, 2024)
stations = ["99495199999", "72429793812"]  # Florida & Cincinnati
```
- Downloads weather CSV files for each year (2021‚Äì2023) for 2 weather stations.
- Files are stored in `/content/weather_data/<year>/`.

---

### **2. CLEAN DATA**
```python
invalid_values = {"MAX": 9999.9, "MXSPD": 999.9}
```
- These represent missing or invalid weather values in NOAA's dataset.
- For each file:
  - It reads the data using Pandas.
  - Filters out rows with invalid values.
  - Saves cleaned data to `/content/cleaned_weather_data/<year>/`.

---

### **3. SPARK SETUP**
```python
spark = SparkSession.builder.appName("Weather Analysis").getOrCreate()
```
- Starts a new Spark session so we can use DataFrames and perform parallel computations.

---

### **4. HOTTEST DAY**
- For each cleaned dataset:
  - Finds the row with the highest `MAX` temperature.
  - Saves results (station, date, max temp) for each year.
- Displays all hottest days.

---

### **5. COLDEST DAY IN MARCH**
- Filters the data for rows in March (`DATE` contains `-03-`).
- Finds the coldest `MIN` temperature day for each dataset.
- Displays the coldest day in March for each year and station.

---

### **6. AVERAGE PRECIPITATION**
- Calculates the average precipitation (`PRCP`) for each station/year.
- Uses Spark‚Äôs aggregation to compute `mean(PRCP)`.
- Displays a table of average rainfall.

---

### **7. WIND CHILL ANALYSIS**
```python
Wind Chill = 35.74 + 0.6215T - 35.75V^0.16 + 0.4275T * V^0.16
```
- Based on temperature (`TEMP`) and wind speed (`WDSP`) for Cincinnati in 2022.
- Applies the formula only if `TEMP < 50` and `WDSP > 3`.
- Displays a list of wind chill values ordered by coldest.

---

### **8. TEMPERATURE PREDICTION MODEL**
- Uses Spark MLlib:
  - Adds `DAY_OF_YEAR` as a feature (1 to 365).
  - Trains a linear regression model to predict `MAX` temperature based on the day of year.
- Predicts temperatures for days 305 to 365 (late-year).
- Displays the predictions.

---

### **9. EXTREME WEATHER DAYS**
```python
df.filter(col("FRSHTT") != 0)
```
- `FRSHTT` is a 6-digit code indicating presence of Fog, Rain, Snow, Hail, Thunder, Tornado.
- Counts how many days had **any** of these extreme conditions in 2023 at the Florida station.
- Prints the total count.

---











Assignment 10:


from pyspark import SparkContext
import kagglehub

def parse_line(line):
    """Parses each line of input data into (movie_id, rating)."""

    if line.startswith("userId,movieId,rating,timestamp"):
        return None
    parts = line.split(",")
    return (int(parts[1]), float(parts[2]))

def main():
    sc = SparkContext("local", "MovieRatings")

    path = kagglehub.dataset_download("rounakbanik/the-movies-dataset")
    dataset_file = f"{path}/ratings.csv"

    input_rdd = sc.textFile(dataset_file)

    mapped_rdd = input_rdd.filter(lambda line: not line.startswith("userId,movieId,rating,timestamp")).map(parse_line)

    reduced_rdd = mapped_rdd.groupByKey().mapValues(lambda ratings: sum(ratings) / len(ratings))

    results = reduced_rdd.collect()
    for movie_id, avg_rating in results:
        print(f"Movie {movie_id} has an average rating of {avg_rating:.2f}")

    sc.stop()

if __name__ == "__main__":
    main()







Explanation:



Sure! Let's go step by step through this PySpark script for calculating **average movie ratings** using a dataset from Kaggle:

---

## üîß **Imports**
```python
from pyspark import SparkContext
import kagglehub
```
- **`SparkContext`**: The main entry point for Spark functionality. It allows interaction with RDDs (Resilient Distributed Datasets).
- **`kagglehub`**: Used to easily download Kaggle datasets in Colab or similar environments.

---

## üß© **Function: `parse_line`**
```python
def parse_line(line):
    """Parses each line of input data into (movie_id, rating)."""
    if line.startswith("userId,movieId,rating,timestamp"):
        return None  # Skip header
    parts = line.split(",")
    return (int(parts[1]), float(parts[2]))  # (movieId, rating)
```
- Ignores the header.
- Splits each CSV line and returns a tuple: **(movie ID, rating)**.

---

## üß† **Main Logic**
```python
def main():
    sc = SparkContext("local", "MovieRatings")  # Start Spark
```
- Starts a **local Spark session** named `"MovieRatings"`.

---

### üì• **Download dataset**
```python
    path = kagglehub.dataset_download("rounakbanik/the-movies-dataset")
    dataset_file = f"{path}/ratings.csv"
```
- Downloads the **"The Movies Dataset"** from Kaggle using `kagglehub`.
- Picks the `ratings.csv` file (which contains user ratings).

---

### üìÑ **Read and Process Data**
```python
    input_rdd = sc.textFile(dataset_file)
```
- Loads the CSV file as an RDD (one line per record).

```python
    mapped_rdd = input_rdd.filter(lambda line: not line.startswith("userId,movieId,rating,timestamp")).map(parse_line)
```
- **Removes the header**.
- **Parses each line** into (movie_id, rating) using `parse_line()`.

---

### üìä **Group and Average Ratings**
```python
    reduced_rdd = mapped_rdd.groupByKey().mapValues(lambda ratings: sum(ratings) / len(ratings))
```
- **Groups ratings by movie ID**.
- **Calculates average** rating for each movie using `sum / count`.

---

### üñ®Ô∏è **Display Results**
```python
    results = reduced_rdd.collect()
    for movie_id, avg_rating in results:
        print(f"Movie {movie_id} has an average rating of {avg_rating:.2f}")
```
- Collects results to the driver and prints average ratings for each movie.

---

### üõë **Stop Spark**
```python
    sc.stop()
```
- Stops the Spark context to free up resources.

---

### ‚úÖ **Entry Point**
```python
if __name__ == "__main__":
    main()
```
- Standard Python entry point. It ensures the script only runs if it's executed directly (not imported).

---

## üß† Summary

This PySpark program:
- Downloads Kaggle's movie rating dataset.
- Parses and cleans data.
- Calculates the **average rating** for each movie using Spark RDDs.
- Displays the results.

---

